This is the R code which serves as the basis for the Bachelor thesis : Governance in the Storm, Forecasting Disaster Casualties and Costs by Nick Bornemann (Leiden University) 



________________________________ Data Merging Phase __________________________

# Load necessary libraries
library(dplyr)
library(readr)
library(dplyr)

# Load datasets (update file paths if necessary)
emdat <- read_csv("EMDATfull.csv")
world_risk <- read_csv("Worldriskindex2000-2024.csv")

# Standardize column names to lowercase for consistency
colnames(emdat) <- tolower(colnames(emdat))
colnames(world_risk) <- tolower(colnames(world_risk))


# Check column names before renaming
print(colnames(emdat))

# Rename relevant EM-DAT columns for clarity (ensure column names exist)
emdat <- dplyr::rename(emdat,
    iso3c = iso,                 
    year = `start year`,         
    total_affected = `total affected`,
    total_damage = `total damage, adjusted ('000 us$)`,  # Ensure this exact column name exists
    aid_received = `ofda/bha response`,
    appeal = `appeal`,
    disaster_type = `disaster type`,
    disaster_group = `disaster subgroup`,
    disaster_id = `disno.`,
    magnitude = magnitude,
    total_deaths = `total deaths`,
    magnitude_scale = `magnitude scale`
  )

# Print column names again to confirm renaming
print(colnames(emdat))


# Ensure emdat is a dataframe
emdat <- as.data.frame(emdat)

# Check column names before selection
print(colnames(emdat))

# Select only columns that actually exist in emdat
valid_columns <- c("iso3c", "year", "total_affected", "total_damage", "aid_received",
                   "appeal", "disaster_type", "disaster_id", "disaster_group", "magnitude", "total_deaths", "magnitude_scale", "region", "subregion" )



emdat <- emdat %>%
  dplyr::select(all_of(intersect(valid_columns, colnames(emdat)))) 

# Print selected column names
print(colnames(emdat))

# Now select only the required columns
emdat <- emdat %>%
  dplyr::select(iso3c, year, total_affected, total_damage, aid_received,
                appeal, disaster_type, disaster_id, magnitude, disaster_group, total_deaths, magnitude_scale, region , subregion)


# Standardize World Risk Index column names (ensure country-year alignment)
world_risk <- world_risk %>%
  dplyr::rename(
    iso3c = iso3.code)

world_risk <- world_risk %>%
  mutate(across(where(is.character), ~ str_replace_all(., ",", "."))) %>%  # replace comma with dot if present
  mutate(across(where(~ all(str_detect(., "^[-+]?[0-9]*\\.?[0-9]+$"), na.rm = TRUE)), as.numeric))


# Merge datasets using a left join (keeping all World Risk Index data)
merged_data <- world_risk %>%
  left_join(emdat, by = c("iso3c", "year"))


merged_data <- merged_data %>%
  mutate(across(where(is.character), ~ str_replace_all(., ",", ".")))


merged_data[merged_data == 0.01] <- NA

# Ensure missing values remain as NA
merged_data[is.na(merged_data)] <- NA

# Print summary of merged dataset
print(summary(merged_data))

na_counts <- colSums(is.na(merged_data))
print(na_counts)

merged_data <- merged_data %>%
  mutate(region_n = as.numeric(factor(region)))

merged_data <- merged_data %>%
  mutate(subregion_n = as.numeric(factor(subregion)))


wb_data <- read_csv("worldb.csv")

print(summary(wb_data))

# Replace ".." with NA in the entire dataset
wb_data[wb_data == ".."] <- NA

# Rename selected variables and convert them to numeric
wb_data <- wb_data %>%
  rename(
    health = `Current health expenditure (% of GDP) [SH.XPD.CHEX.GD.ZS]`,
    military    = `Military expenditure (% of GDP) [MS.MIL.XPND.GD.ZS]`,
    slum   = `Population living in slums (% of urban population) [EN.POP.SLUM.UR.ZS]`,
    urban         = `Urban population (% of total population) [SP.URB.TOTL.IN.ZS]`,
    gini    = `Gini index [SI.POV.GINI]`,
    effectivness      = `Government Effectiveness: Estimate [GE.EST]`,
    mobile      = `Mobile cellular subscriptions (per 100 people) [IT.CEL.SETS.P2]`,
    internet      = `Individuals using the Internet (% of population) [IT.NET.USER.ZS]`,
    education      = `Government expenditure on education, total (% of GDP) [SE.XPD.TOTL.GD.ZS]`,
    hospital      = `Hospital beds (per 1,000 people) [SH.MED.BEDS.ZS]`,
    life      = `Life expectancy at birth, total (years) [SP.DYN.LE00.IN]`,
    gdp      = `GDP per capita, PPP (current international $) [NY.GDP.PCAP.PP.CD]`,
    education2      = `Compulsory education, duration (years) [SE.COM.DURS]`, 
    corruption      = `Control of Corruption: Estimate [CC.EST]`
  ) %>%
  mutate(across(c(health, military, slum, urban, gini, effectivness, mobile, internet, education, education2, hospital, life, gdp, corruption), as.numeric))


colnames(wb_data) <- tolower(colnames(wb_data))

# Standardize WBData column names for consistency
wb_data <- wb_data %>%
  rename(
    iso3c = `country code`,
    year = time
  )
print(summary(wb_data))

# Ensure WBData is a dataframe
wb_data <- as.data.frame(wb_data)

print(summary(wb_data))

# Ensure 'year' is numeric in both datasets before merging
wb_data <- wb_data %>%
  mutate(year = as.numeric(year))

# Merge with WBData (left join)
merged_data <- merged_data %>%
  left_join(wb_data, by = c("iso3c", "year"))

# Ensure missing values remain as NA
merged_data[is.na(merged_data)] <- NA

# Print summary of final merged dataset
print(summary(merged_data))

na_counts <- colSums(is.na(merged_data))
print(na_counts)


# Optional: Check remaining variables
print(names(merged_data))

# Save the final merged dataset
write_csv(merged_data, "merged_emdat_worldrisk_wbdata.csv")

________________________________ Data Cleaning and Preperation  __________________________

# Load necessary libraries
library(dplyr)
library(readr)
library(caret)
library(randomForest)

# Load the merged dataset
data <- read_csv("merged_emdat_worldrisk_wbdata.csv")


data <- data %>%
  mutate(
    corruption_scaled = (corruption - min(corruption, na.rm = TRUE)) /
      (max(corruption, na.rm = TRUE) - min(corruption, na.rm = TRUE)),
    
    effectiveness_scaled = (effectivness - min(effectivness, na.rm = TRUE)) /
      (max(effectivness, na.rm = TRUE) - min(effectivness, na.rm = TRUE))
  )


data <- data %>%
  dplyr::rename(
    dis_typ = disaster_type,
    disaster_type = disaster_group
  )

library(fastDummies)

data <- dummy_cols(data, select_columns = "disaster_type", remove_first_dummy = TRUE)


# Convert back to factor if needed
data <- data %>%
  mutate(disaster_type = as.factor(disaster_type))

# View updated data
summary(data$disaster_type)

library(dplyr)
library(stringr)

data <- data %>%
  mutate(across(where(is.character), ~ str_replace_all(., ",", "."))) %>%  # Replace commas with dots
  mutate(across(where(~ all(str_detect(., "^[-+]?[0-9]*\\.?[0-9]+$"), na.rm = TRUE)), as.numeric))  # Convert valid numerics to numbers


summary(data)
data.frame(Variable = names(data), NAs = colSums(is.na(data)))

# Exclude specific variables
data <- data[, !(names(data) %in% c("si_14a", "disaster_type_Extra.terrestrial", "r_d", "slum", "fixed telephone subscriptions (per 100 people) [it.mlt.main.p2]", "military" ))]

# Save the cleaned dataset to a CSV file
write.csv(data, "cleaned_data.csv", row.names = FALSE)

# Read the CSV back into R and overwrite 'data'
data <- read.csv("cleaned_data.csv")

# Dataset done 



col_info <- data.frame(Column_Number = seq_along(names(data)), Column_Name = names(data))
print(col_info)


print(summary(data))

# Step 1: Identify numeric columns to lag (excluding target vars)
num_cols <- names(data)[sapply(data, is.numeric)]
targets <- c("total_affected", "total_damage", "total_deaths", "disaster_type_Geophysical", "disaster_type_Hydrological", "disaster_type_Meteorological", "disaster_type_NA", "subregion_n", "region_n", "region", "subregion")
to_lag <- setdiff(num_cols, targets)

# Step 2: Lag them by country
data_lagged <- data %>%
  arrange(iso3c, year) %>%
  group_by(iso3c) %>%
  mutate(across(all_of(to_lag), ~ lag(.x, 1), .names = "{.col}_lag")) %>%
  ungroup()

data_lagged <- data_lagged %>%
  dplyr::select(
    iso3c, year, ends_with("_lag"),
    disaster_id, disaster_type,
    total_affected, total_damage, total_deaths, appeal, dis_typ
  ) 



# Create a dataset with all columns but only rows where total_deaths is not NA
data_total_deaths <- data_lagged %>%
  filter(!is.na(total_deaths))

# Create a dataset with all columns but only rows where total_damage is not NA
data_total_damage <- data_lagged %>%
  filter(!is.na(total_damage))

# Print dimensions to check results
print(dim(data_total_deaths))
print(dim(data_total_damage))

# Optionally, save them as new datasets
write.csv(data_total_deaths, "data_total_deaths.csv", row.names = FALSE)
write.csv(data_total_damage, "data_total_damage.csv", row.names = FALSE)

data.frame(Variable = names(data_total_deaths), NAs = colSums(is.na(data_total_deaths)))



# Read the dataset
data_total_deaths <- read.csv("data_total_deaths.csv")



qqnorm(data_total_deaths$total_deaths,
       main = "Q-Q Plot of Total Deaths")
qqline(data_total_deaths$total_deaths, col = "red")

plot(density(data_total_deaths$total_deaths, na.rm = TRUE),
     main = "Density Plot of Total Deaths",
     xlab = "Total Deaths")

# Step 1: KEEP everything (including 0 deaths!)
# No filtering for deaths >= 10 anymore
filtered_deaths <- data_total_deaths  # <-- keep the full dataset

# Step 2: Calculate the 90th percentile to trim top 10% (optional)
upper_bound_deaths <- quantile(filtered_deaths$total_deaths, 0.95, na.rm = TRUE)

# Step 3: Exclude top 10% of deaths (only the extreme high values)
data_total_deaths_trimmed <- filtered_deaths %>%
  filter(total_deaths <= upper_bound_deaths)

# Step 4: Plots for trimmed data

# Q-Q plot
qqnorm(data_total_deaths_trimmed$total_deaths,
       main = "Q-Q Plot of Total Deaths (Trimmed)")
qqline(data_total_deaths_trimmed$total_deaths, col = "red")

# Density plot
plot(density(data_total_deaths_trimmed$total_deaths, na.rm = TRUE),
     main = "Density Plot of Total Deaths (Trimmed)",
     xlab = "Total Deaths")



# Min-Max scaling function
min_max_scale <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# Apply scaling
data_total_deaths_trimmed[cols_to_scale] <- lapply(data_total_deaths_trimmed[cols_to_scale], min_max_scale)

# Check the result
summary(data_total_deaths_trimmed)

# Step 3: Apply Min-Max scaling to the log-transformed variable
data_total_deaths_10$log_total_deaths_scaled <- min_max_scale(data_total_deaths_10$log_total_deaths)


# Identify _lag variables
lag_vars <- grep("_lag$", names(data_total_deaths_10), value = TRUE)


# Save the imputed dataset
write.csv(data_total_deaths_trimmed, "data_total_deaths_imputed.csv", row.names = FALSE)

data_total_deaths_imputed <- read.csv("data_total_deaths_imputed.csv")

##### Done with total deaths ! 






# Read the dataset
data_total_damage <- read.csv("data_total_damage.csv")

# Step 1: Filter values greater than 50,000
filtered_data <- data_total_damage[data_total_damage$total_damage > 50000, ]

# Step 2: Calculate the 90th percentile of total_damage
upper_bound <- quantile(filtered_data$total_damage, 0.95, na.rm = TRUE)

# Step 3: Exclude values above the 99th percentile
data_total_damage_50 <- filtered_data[filtered_data$total_damage <= upper_bound, ]

qqnorm(data_total_damage_50$total_damage)
qqline(data_total_damage_50$total_damage, col = "red")

plot(density(data_total_damage_50$total_damage, na.rm = TRUE),
     main = "Density Plot of Total Damage (Filtered)",
     xlab = "Total Damage")

# Step 1: Log-transform total_damage (safely)
data_total_damage_50$log_total_damage <- log1p(data_total_damage_50$total_damage)

plot(density(data_total_damage_50$total_damage, na.rm = TRUE),
     main = "Density Plot of Total Damage (Filtered)",
     xlab = "Total Damage")

# Step 2: Reuse the same Min-Max scaling function
min_max_scale <- function(x) {
  (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
}

# Step 3: Apply Min-Max scaling to the log-transformed variable
data_total_damage_50$log_total_damage_scaled <- min_max_scale(data_total_damage_50$log_total_damage)


######### Death Model '#############



library(caret)
death_run <- read.csv("data_total_deaths_imputed.csv")


# Calculate min and max
min_val <- min(death_run$total_deaths, na.rm = TRUE)
max_val <- max(death_run$total_deaths, na.rm = TRUE)

# Min-Max scale to [0, 1]
death_run <- death_run %>%
  mutate(total_deaths_scaled = (total_deaths - min_val) / (max_val - min_val))

# Check the result
summary(death_run$total_deaths_scaled)

# Split scaled total_deaths into 3 groups: Low (0-0.33], Medium (0.33-0.66], High (0.66-1]
death_run$deaths_group <- cut(
  death_run$total_deaths_scaled,
  breaks = c(-Inf, 1/4, 2/4, Inf),
  labels = c("Low", "Medium", "High"),
  right = TRUE
)

# Check the distribution
table(death_run$deaths_group)


library(ggplot2)

# Make sure deaths_group is a factor (for nice ordering)
death_run$deaths_group <- factor(death_run$deaths_group, levels = c("Low", "Medium", "High"))

# Plot
ggplot(death_run, aes(x = total_deaths)) +
  geom_histogram(bins = 30, fill = "#c7dcff", color = "black") +
  facet_wrap(~ deaths_group, scales = "free_y") +
  labs(
    title = "Distribution of Scaled Total Deaths by Group",
    x = "Scaled Total Deaths (0-1)",
    y = "Count"
  ) +
  theme_minimal(base_family = "Times", base_size = 12) +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    strip.text = element_text(face = "bold"),
    axis.text.x = element_text(angle = 0, hjust = 0.5)
  )



library(dplyr)
library(caret)
library(ggplot2)

#─── 1. Selected predictors ───────────────────────────────────────────────────
selected_vars_death <- c("ai_01a_lag", "ai_02a_lag", "ai_02b_lag",
                   "ci_03a_lag", "ci_03b_lag", "ci_04a_lag", "ci_04b_lag",
                   "ci_06a_lag", "s_05_lag", "si_01a_lag", "si_02a_lag",
                   "si_02b_lag", "si_03a_lag", "si_05a_lag", "si_07b_lag",
                   "si_08a_lag", "si_09a_lag", "internet_lag", "gdp_lag", "life_lag","corruption_lag", 
                   "health_lag", "hospital_lag", "urban_lag", "ai_03c_lag", "exposure_lag")

data_death_clean <- death_run %>% 
  dplyr::select(all_of(c(selected_vars, "deaths_group", "total_deaths"))) %>% 
  na.omit()

#─── 2. Subset clean data by deaths group ──────────────────────────────────────
data_death_clean$deaths_group <- as.numeric(as.factor(data_death_clean$deaths_group))

# Now subset based on deaths_group
death_group_1 <- subset(data_death_clean, deaths_group == 1) # Low
death_group_2 <- subset(data_death_clean, deaths_group == 2) # Medium
death_group_3 <- subset(data_death_clean, deaths_group == 3) # High

subsets_death <- list(
  Low = death_group_1,
  Medium = death_group_2,
  High = death_group_3
)

#─── 3. Evaluation helper ──────────────────────────────────────────────────────
evaluate_model_death <- function(actual, predicted) {
  r_squared <- cor(actual, predicted, use = "complete.obs")^2
  rmse <- sqrt(mean((actual - predicted)^2))
  return(c(R2 = round(r_squared, 3), RMSE = round(rmse, 3)))
}

#─── 4. Model pipeline ─────────────────────────────────────────────────────────
run_model_pipeline_death <- function(df, name = "Dataset") {
  if (nrow(df) < 10) {
    warning(paste("Skipping", name, "- not enough data"))
    return(data.frame(
      DeathGroup = name,
      Model = c("OLS", "kNN", "RF", "GBM"),
      RMSE = NA,
      R2 = NA,
      N_train = NA,
      N_test = NA
    ))
  }
  
  df[selected_vars_death] <- lapply(df[selected_vars_death], as.numeric)
  
  set.seed(123)
  idx <- createDataPartition(df$total_deaths, p = 0.7, list = FALSE)
  train_data <- df[idx, ]
  test_data <- df[-idx, ]
  form <- as.formula(paste("total_deaths ~", paste(selected_vars_death, collapse = " + ")))
  ctrl <- trainControl(method = "cv", number = 5)
  
  tryCatch({
    # Models
    ols_mod <- lm(form, data = train_data)
    ols_pred <- predict(ols_mod, newdata = test_data)
    ols_res <- evaluate_model_death(test_data$total_deaths, ols_pred)
    
    knn_mod <- caret::train(form, data = train_data, method = "knn", tuneLength = 10)
    knn_pred <- predict(knn_mod, newdata = test_data)
    knn_res <- evaluate_model_death(test_data$total_deaths, knn_pred)
    
    rf_mod <- caret::train(form, data = train_data, method = "rf", trControl = ctrl, tuneLength = 5)
    rf_pred <- predict(rf_mod, newdata = test_data)
    rf_res <- evaluate_model_death(test_data$total_deaths, rf_pred)
    
    gbm_mod <- caret::train(form, data = train_data, method = "gbm", trControl = ctrl, tuneLength = 5, verbose = FALSE)
    gbm_pred <- predict(gbm_mod, newdata = test_data)
    gbm_res <- evaluate_model_death(test_data$total_deaths, gbm_pred)
    
    # Print results
    cat("\nModel Performance for", name, "\n")
    cat(sprintf("N_train: %d | N_test: %d\n", nrow(train_data), nrow(test_data)))
    cat(sprintf("OLS - R²: %.3f | RMSE: %.3f\n", ols_res["R2"], ols_res["RMSE"]))
    cat(sprintf("kNN - R²: %.3f | RMSE: %.3f\n", knn_res["R2"], knn_res["RMSE"]))
    cat(sprintf("RF  - R²: %.3f | RMSE: %.3f\n", rf_res["R2"], rf_res["RMSE"]))
    cat(sprintf("GBM - R²: %.3f | RMSE: %.3f\n", gbm_res["R2"], gbm_res["RMSE"]))
    
    data.frame(
      DeathGroup = name,
      Model = c("OLS", "kNN", "RF", "GBM"),
      RMSE = c(ols_res["RMSE"], knn_res["RMSE"], rf_res["RMSE"], gbm_res["RMSE"]),
      R2 = c(ols_res["R2"], knn_res["R2"], rf_res["R2"], gbm_res["R2"]),
      N_train = nrow(train_data),
      N_test = nrow(test_data)
    )
  }, error = function(e) {
    warning(paste("Model failed for", name, ":", e$message))
    data.frame(
      DeathGroup = name,
      Model = c("OLS", "kNN", "RF", "GBM"),
      RMSE = NA,
      R2 = NA,
      N_train = NA,
      N_test = NA
    )
  })
}

rmse_list_death <- lapply(names(subsets_death), function(group) {
  run_model_pipeline_death(subsets_death[[group]], paste0("death_group_", group))
})
rmse_df_death <- do.call(rbind, rmse_list_death)


library(caret)

# Train the kNN model
knn_mod <- caret::train(
  form_death,
  data = train_death,
  method = "knn",
  tuneLength = 10  # tries 10 values of k
)

# Check best k
knn_mod$bestTune



library(caret)

# Cross-validation control
ctrl <- trainControl(method = "cv", number = 5)

# Grid of mtry values to try
rf_grid <- expand.grid(mtry = c(5, 10, 15))

# Train Random Forest model
rf_mod <- caret::train(
  form_death,
  data = train_death,
  method = "rf",
  trControl = ctrl,
  tuneGrid = rf_grid,
  importance = TRUE
)

# View the best mtry value
rf_mod$bestTune

# Print directly
cat("Best mtry for Random Forest:", rf_mod$bestTune$mtry, "\n")



#─── 6. Plot RMSEs ─────────────────────────────────────────────────────────────
ggplot(rmse_df_death, aes(x = Model, y = RMSE, color = Model)) +
  geom_point(size = 4) +
  facet_wrap(~ DeathGroup) +
  labs(
    title = "Model RMSEs by Death Group (Death Data)",
    x = "Model",
    y = "RMSE"
  ) +
  theme_minimal()

# Ensure correct order and naming of DeathGroup
rmse_df_death$DeathGroup <- factor(
  rmse_df_death$DeathGroup,
  levels = c("death_group_Low", "death_group_Medium", "death_group_High"),
  labels = c("Low", "Medium", "High")
)

# Plot R² vs RMSE
ggplot(rmse_df_death, aes(x = R2, y = RMSE,color = Model)) +
  geom_point(size = 5) + 
  facet_wrap(~ DeathGroup, nrow = 1) +
  labs(
    title = "Model Performance: R² vs RMSE by Death Group",
    x = "R²",
    y = "RMSE"
  ) +
  theme_minimal(base_family = "Times") +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    strip.text = element_text(face = "bold")
  )


library(ggplot2)
library(ggrepel)

ggplot(rmse_df_death, aes(x = R2, y = RMSE, color = Model, shape = Model)) +
  geom_point(size = 5) +
  geom_text_repel(aes(label = Model), size = 3, color = "gray30") +
  scale_shape_manual(values = c(16, 17, 15, 18)) +
  scale_color_manual(
    values = c(
      "OLS" = "#f6765b",
      "kNN" = "#c7dcff",
      "RF"  = "#FABD05",
      "GBM" = "#ffe1fc"
    )
  ) +
  facet_wrap(~ DeathGroup, nrow = 1) +
  labs(
    title = "Model Performance: R² vs RMSE by Death Group",
    x = "R² (Higher is Better)",
    y = "RMSE (Lower is Better)"
  ) +
  theme_minimal(base_family = "Times") +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    strip.text = element_text(face = "bold", size = 12),
    axis.title = element_text(face = "bold"),
    legend.position = "bottom"
  )

######done for the first part 1/3 split####

library(caret)

#──────────────────────────────────────────────────────────────────────────────
# 1. Prep Full Data
#──────────────────────────────────────────────────────────────────────────────
set.seed(42)
df_death <- data_death_clean
df_death[selected_vars_death] <- lapply(df_death[selected_vars_death], as.numeric)

# Train/test split
idx_death <- createDataPartition(df_death$total_deaths, p = 0.7, list = FALSE)
train_death <- df_death[idx_death, ]
test_death <- df_death[-idx_death, ]

form_death <- as.formula(paste("total_deaths ~", paste(selected_vars_death, collapse = " + ")))

#──────────────────────────────────────────────────────────────────────────────
# 2. Train Hyperparameter-Tuned RF Model
#──────────────────────────────────────────────────────────────────────────────
rf_grid <- expand.grid(mtry = c(5, 10, 15))
ctrl <- trainControl(method = "cv", number = 5)

rf_death <- caret::train(
  form_death,
  data = train_death,
  method = "rf",
  trControl = ctrl,
  tuneGrid = rf_grid,
  importance = TRUE
)

# Predict & evaluate
rf_pred_death <- predict(rf_death, newdata = test_death)
evaluate_model <- function(actual, predicted) {
  r_squared <- cor(actual, predicted, use = "complete.obs")^2
  rmse <- sqrt(mean((actual - predicted)^2))
  return(c(R2 = round(r_squared, 3), RMSE = round(rmse, 3)))
}
eval_death <- evaluate_model(test_death$total_deaths, rf_pred_death)
print(eval_death)


comparison_df <- data.frame(
  Actual = test_death$total_deaths,
  Predicted = rf_pred_death
)

ggplot(comparison_df, aes(x = Actual, y = Predicted)) +
  geom_point(shape = 21, fill = "#c7dcff", color = "gray40", stroke = 0.5, alpha = 0.7, size = 3) +
  geom_smooth(method = "lm", color = "#EA4336", se = FALSE, linetype = "dashed") +
  geom_abline(slope = 1, intercept = 0, color = "gray50", linetype = "dotted") +
  labs(
    title = "Predicted vs Actual Total Deaths",
    x = "Actual (total deaths)",
    y = "Predicted (total deaths)"
  ) +
  theme_minimal(base_family = "Times") +
  theme(
    plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
    axis.title = element_text(face = "bold")
  )


rf_imp_death <- varImp(rf_death, scale = TRUE)
rf_imp_df <- rf_imp_death$importance %>%
  rownames_to_column("Variable") %>%
  arrange(desc(Overall))

ggplot(rf_imp_df, aes(x = reorder(Variable, Overall), y = Overall, fill = Overall)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = round(Overall, 2)), hjust = -0.2, size = 3.5, color = "gray20") +
  scale_fill_gradient(low = "#FABD05", high = "#EA4336") +
  coord_flip() +
  labs(
    title = "Random Forest Variable Importance (Death Model)",
    x = "",
    y = "Scaled Importance"
  ) +
  theme_minimal(base_family = "Times") +
  theme(
    plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
    axis.title.y = element_blank(),
    axis.text.y = element_text(size = 10)
  ) +
  ylim(0, max(rf_imp_df$Overall) * 1.15)


library(iml)

X_death <- train_death[, selected_vars_death]
y_death <- train_death$total_deaths
predictor_death <- Predictor$new(rf_death, data = X_death, y = y_death)

shap_death <- FeatureImp$new(predictor_death, loss = "mae")



# SHAP: recompute for a small sample
library(iml)
library(dplyr)
library(ggplot2)

# Sample (e.g. 100 observations)
X_sample <- X_death[1:100, ]
shapley_list <- lapply(1:nrow(X_sample), function(i) {
  Shapley$new(predictor_death, x.interest = X_sample[i, ])$results %>%
    mutate(instance = i)
})
shap_long <- do.call(rbind, shapley_list)

# Violin Plot
ggplot(shap_long, aes(x = phi, y = feature, fill = feature)) +
  geom_violin(trim = FALSE, scale = "width", color = NA, alpha = 0.8) +
  geom_vline(xintercept = 0, linetype = "dotted", color = "gray50") +
  labs(
    title = "SHAP Value Distribution per Feature (Death Model)",
    x = "SHAP Value (Impact on Prediction)",
    y = NULL
  ) +
  theme_minimal(base_family = "Times") +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.text.y = element_text(size = 9)
  ) +
  scale_fill_manual(values = rep(c("#f6765b", "#c7dcff", "#FABD05", "#ffe1fc"), length.out = length(unique(shap_long$feature))))

set.seed(42)
n_permutations <- 30
r2_null <- numeric(n_permutations)

for (i in 1:n_permutations) {
  # Permute target
  train_death_permuted <- train_death
  train_death_permuted$total_deaths <- sample(train_death$total_deaths)
  
  # Train RF on permuted target
  rf_null <- caret::train(
    form_death,
    data = train_death_permuted,
    method = "rf",
    trControl = ctrl,
    tuneGrid = rf_grid,
    importance = TRUE
  )
  
  # Predict on the *real* test set
  preds_null <- predict(rf_null, newdata = test_death)
  
  # Evaluate performance
  r2_null[i] <- cor(preds_null, test_death$total_deaths, use = "complete.obs")^2
}

# Compare against real model R²
real_r2 <- eval_death["R2"]
cat("Real model R²:", real_r2, "\n")
cat("Null mean R²:", mean(r2_null), "\n")

# Plot distribution
library(ggplot2)
df_r2 <- data.frame(R2 = r2_null)

# Create density object manually to get coordinates for text
dens <- density(r2_null)

# Build dataframe for the density
df_dens <- data.frame(x = dens$x, y = dens$y)

# Plot
ggplot(df_dens, aes(x = x, y = y)) +
  geom_area(fill = "#c7dcff", alpha = 0.8, color = "gray40", linewidth = 0.5) +
  geom_vline(xintercept = real_r2, color = "#EA4336", linetype = "dashed", linewidth = 1.2) +
  geom_text(
    aes(x = real_r2, y = max(y) * 0.9),
    label = paste("Real R² =", round(real_r2, 3)),
    hjust = -0.1,
    color = "#EA4336",
    fontface = "bold",
    size = 4
  ) +
  labs(
    title = "Permutation Test: R² of Null Models vs Real Model",
    subtitle = paste0("Real R² = ", round(real_r2, 3),
                      " | Null mean R² = ", round(mean(r2_null), 3)),
    x = "R² from Permuted Targets",
    y = "Density"
  ) +
  theme_minimal(base_family = "Times") +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 10, hjust = 0.5),
    axis.title = element_text(face = "bold")
  )



######## Cost Model ############


# Load libraries
library(dplyr)
library(ggplot2)
library(caret)

# Load dataset
data_cost <- read.csv("data_total_damage_imputed.csv")

#─── 1. Scale log_total_damage ────────────────────────────────────────────────

# Calculate min and max
min_val <- min(data_cost$log_total_damage, na.rm = TRUE)
max_val <- max(data_cost$log_total_damage, na.rm = TRUE)

# Min-Max scale to [0, 1]
data_cost <- data_cost %>%
  mutate(log_total_damage_scaled = (log_total_damage - min_val) / (max_val - min_val))

# Check the result
summary(data_cost$log_total_damage_scaled)

#─── 2. Categorize into groups ─────────────────────────────────────────────────

# Split scaled log_total_damage into 3 groups: Low, Medium, High
data_cost$damage_group <- cut(
  data_cost$log_total_damage_scaled,
  breaks = c(-Inf, 1/3, 2/3, Inf),
  labels = c("Low", "Medium", "High"),
  right = TRUE
)

# Check group distribution
table(data_cost$damage_group)

#─── 3. Plot histogram by group ────────────────────────────────────────────────

# Ensure factor levels are in order
data_cost$damage_group <- factor(data_cost$damage_group, levels = c("Low", "Medium", "High"))

ggplot(data_cost, aes(x = log_total_damage_scaled)) +
  geom_histogram(bins = 30, fill = "#f6e7ff", color = "black") +
  facet_wrap(~ damage_group, scales = "free_y") +
  labs(
    title = "Distribution of Scaled Log Total Damage by Group",
    x = "Log Total Damage",
    y = "Count"
  ) +
  theme_minimal(base_family = "Times", base_size = 12) +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    strip.text = element_text(face = "bold"),
    axis.text.x = element_text(angle = 0, hjust = 0.5)
  )

#─── 4. Select predictors and clean ────────────────────────────────────────────

selected_vars <- c("ai_01a_lag", "ai_02a_lag", "ai_02b_lag",
                   "ci_03a_lag", "ci_03b_lag", "ci_04a_lag", "ci_04b_lag",
                   "ci_06a_lag", "s_05_lag", "si_01a_lag", "si_02a_lag",
                   "si_02b_lag", "si_03a_lag", "si_05a_lag", "si_07b_lag",
                   "si_08a_lag", "si_09a_lag", "internet_lag", "gdp_lag", "life_lag","corruption_lag", 
                   "health_lag", "hospital_lag", "urban_lag", "ai_03c_lag", "exposure_lag")

data_cost_clean <- data_cost %>% 
  dplyr::select(all_of(c(selected_vars, "damage_group", "log_total_damage"))) %>% 
  na.omit()

# Convert factor to numeric
data_cost_clean$damage_group <- as.numeric(as.factor(data_cost_clean$damage_group))

#─── 5. Create subsets ─────────────────────────────────────────────────────────

damage_group_1 <- subset(data_cost_clean, damage_group == 1) # Low
damage_group_2 <- subset(data_cost_clean, damage_group == 2) # Medium
damage_group_3 <- subset(data_cost_clean, damage_group == 3) # High

subsets_cost <- list(
  Low = damage_group_1,
  Medium = damage_group_2,
  High = damage_group_3
)


#─── 5. Evaluation helper ──────────────────────────────────────────────────────
evaluate_model <- function(actual, predicted) {
  r_squared <- cor(actual, predicted, use = "complete.obs")^2
  rmse <- sqrt(mean((actual - predicted)^2))
  return(c(R2 = round(r_squared, 3), RMSE = round(rmse, 3)))
}

#─── 6. Model pipeline ─────────────────────────────────────────────────────────
run_model_pipeline <- function(df, name = "Dataset") {
  if (nrow(df) < 10) {
    warning(paste("Skipping", name, "- not enough data"))
    return(data.frame(
      DamageGroup = name,
      Model = c("OLS", "kNN", "RF", "GBM"),
      RMSE = NA,
      R2 = NA,
      N_train = NA,
      N_test = NA
    ))
  }
  
  df[selected_vars] <- lapply(df[selected_vars], as.numeric)
  
  set.seed(123)
  idx <- createDataPartition(df$log_total_damage, p = 0.7, list = FALSE)
  train_data <- df[idx, ]
  test_data <- df[-idx, ]
  form <- as.formula(paste("log_total_damage ~", paste(selected_vars, collapse = " + ")))
  ctrl <- trainControl(method = "cv", number = 5)
  
  tryCatch({
    # Models
    ols_mod <- lm(form, data = train_data)
    ols_pred <- predict(ols_mod, newdata = test_data)
    ols_res <- evaluate_model(test_data$log_total_damage, ols_pred)
    
    knn_mod <- caret::train(form, data = train_data, method = "knn", tuneLength = 10)
    knn_pred <- predict(knn_mod, newdata = test_data)
    knn_res <- evaluate_model(test_data$log_total_damage, knn_pred)
    
    rf_mod <- caret::train(form, data = train_data, method = "rf", trControl = ctrl, tuneLength = 5)
    rf_pred <- predict(rf_mod, newdata = test_data)
    rf_res <- evaluate_model(test_data$log_total_damage, rf_pred)
    
    gbm_mod <- caret::train(form, data = train_data, method = "gbm", trControl = ctrl, tuneLength = 5, verbose = FALSE)
    gbm_pred <- predict(gbm_mod, newdata = test_data)
    gbm_res <- evaluate_model(test_data$log_total_damage, gbm_pred)
    
    # Print results
    cat("\nModel Performance for", name, "\n")
    cat(sprintf("N_train: %d | N_test: %d\n", nrow(train_data), nrow(test_data)))
    cat(sprintf("OLS - R²: %.3f | RMSE: %.3f\n", ols_res["R2"], ols_res["RMSE"]))
    cat(sprintf("kNN - R²: %.3f | RMSE: %.3f\n", knn_res["R2"], knn_res["RMSE"]))
    cat(sprintf("RF  - R²: %.3f | RMSE: %.3f\n", rf_res["R2"], rf_res["RMSE"]))
    cat(sprintf("GBM - R²: %.3f | RMSE: %.3f\n", gbm_res["R2"], gbm_res["RMSE"]))
    
    data.frame(
      DamageGroup = name,
      Model = c("OLS", "kNN", "RF", "GBM"),
      RMSE = c(ols_res["RMSE"], knn_res["RMSE"], rf_res["RMSE"], gbm_res["RMSE"]),
      R2 = c(ols_res["R2"], knn_res["R2"], rf_res["R2"], gbm_res["R2"]),
      N_train = nrow(train_data),
      N_test = nrow(test_data)
    )
  }, error = function(e) {
    warning(paste("Model failed for", name, ":", e$message))
    data.frame(
      DamageGroup = name,
      Model = c("OLS", "kNN", "RF", "GBM"),
      RMSE = NA,
      R2 = NA,
      N_train = NA,
      N_test = NA
    )
  })
}

#─── 7. Collect RMSEs from all subsets ────────────────────────────────────────
rmse_list_cost <- lapply(names(subsets_cost), function(group) {
  run_model_pipeline(subsets_cost[[group]], paste0("cost_group_", group))
})
rmse_df_cost <- do.call(rbind, rmse_list_cost)


# Sort groups as Low → Medium → High
rmse_df_cost$DamageGroup <- factor(
  rmse_df_cost$DamageGroup,
  levels = c("cost_group_Low", "cost_group_Medium", "cost_group_High"),
  labels = c("Low", "Medium", "High")
)




ggplot(rmse_df_cost, aes(x = R2, y = RMSE, color = Model)) +
  geom_point(size = 5) +
  scale_shape_manual(values = c(16, 17, 15, 18)) +
  facet_wrap(~ DamageGroup, nrow = 1) +
  labs(
    title = "Model Performance: R² vs RMSE by Damage Group",
    x = "R²",
    y = "RMSE"
  ) +
  theme_minimal(base_family = "Times") +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    strip.text = element_text(face = "bold"),
    legend.position = "bottom"
  )

exp(0.6)  # Example: log-scale RMSE = 0.5

library(ggplot2)
library(ggrepel)

ggplot(rmse_df_cost, aes(x = R2, y = RMSE, color = Model, shape = Model)) +
  geom_point(size = 5) +
  geom_text_repel(aes(label = Model), size = 3, color = "gray30") +
  scale_shape_manual(values = c(16, 17, 15, 18)) +
  scale_color_manual(
    values = c(
      "OLS" = "#f6765b",
      "kNN" = "#c7dcff",
      "RF"  = "#FABD05",
      "GBM" = "#ffe1fc"
    )
  ) +
  facet_wrap(~ DamageGroup, nrow = 1) +
  labs(
    title = "Model Performance: R² vs RMSE by Damage Group",
    x = "R² (Higher is Better)",
    y = "RMSE (Lower is Better)"
  ) +
  theme_minimal(base_family = "Times") +
  theme(
    plot.title = element_text(size = 16, face = "bold", hjust = 0.5),
    strip.text = element_text(face = "bold", size = 12),
    axis.title = element_text(face = "bold"),
    legend.position = "bottom"
  )





#─── 1. Prep Full Data ────────────────────────────────────────────────────────
set.seed(42)
df_all <- data_cost_clean
df_all[selected_vars] <- lapply(df_all[selected_vars], as.numeric)

# Train/test split
idx_all <- createDataPartition(df_all$log_total_damage, p = 0.7, list = FALSE)
train_all <- df_all[idx_all, ]
test_all <- df_all[-idx_all, ]

form <- as.formula(paste("log_total_damage ~", paste(selected_vars, collapse = " + ")))

#─── 2. Hyperparameter-Tuned Random Forest ────────────────────────────────────
rf_grid <- expand.grid(mtry = c(5, 10, 15))
ctrl <- trainControl(method = "cv", number = 5)

rf_all <- train(
  form,
  data = train_all,
  method = "rf",
  trControl = ctrl,
  tuneGrid = rf_grid,
  importance = TRUE
)

# Evaluation
rf_pred_all <- predict(rf_all, newdata = test_all)
eval_all <- evaluate_model(test_all$log_total_damage, rf_pred_all)
print(eval_all)

library(ggplot2)

# Create dataframe for plotting
comparison_df <- data.frame(
  Actual = test_all$log_total_damage,
  Predicted = rf_pred_all
)

# Plot
ggplot(comparison_df, aes(x = Actual, y = Predicted)) +
  geom_point(shape = 21, fill = "#f6e7ff", color = "gray40", stroke = 0.5, alpha = 0.7, size = 3) +
  geom_smooth(method = "lm", color = "#EA4336", se = FALSE, linetype = "dashed") +
  geom_abline(slope = 1, intercept = 0, color = "gray50", linetype = "dotted") +
  labs(
    title = "Predicted vs Actual Log Total Damage",
    x = "Actual (log total damage)",
    y = "Predicted (log total damage)"
  ) +
  theme_minimal(base_family = "Times") +
  theme(
    plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
    axis.title = element_text(face = "bold")
  )


library(ggplot2)
library(scales)

rf_imp_cost <- varImp(rf_all, scale = TRUE)
rf_imp_cost <- rf_imp_cost$importance %>%
  rownames_to_column("Variable") %>%
  arrange(desc(Overall))

# Fancy importance plot
ggplot(rf_imp_cost, aes(x = reorder(Variable, Overall), y = Overall, fill = Overall)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = round(Overall, 2)), hjust = -0.2, size = 3.5, color = "gray20") +
  scale_fill_gradient(low = "#FABD05", high = "#EA4336") +
  coord_flip() +
  labs(
    title = "Random Forest Variable Importance (Cost Model)",
    x = "",
    y = "Scaled Importance"
  ) +
  theme_minimal(base_family = "Times") +
  theme(
    plot.title = element_text(face = "bold", size = 14, hjust = 0.5),
    axis.title.y = element_blank(),
    axis.text.y = element_text(size = 10)
  ) +
  ylim(0, max(rf_imp_df$Overall) * 1.15)




library(iml)

# Create predictor object
X_all <- train_all[, selected_vars]
y_all <- train_all$log_total_damage
predictor_all <- Predictor$new(rf_all, data = X_all, y = y_all)

# SHAP-like permutation importance
shap_all <- FeatureImp$new(predictor_all, loss = "mae")

# Plot
plot(shap_all) +
  ggtitle("SHAP-like Feature Importance (Permutation on Full Dataset)") +
  theme_minimal(base_family = "Times")





library(iml)
library(dplyr)
library(ggplot2)

# Create predictor object for cost model
X_cost <- train_all[, selected_vars]
y_cost <- train_all$log_total_damage
predictor_cost <- Predictor$new(rf_all, data = X_cost, y = y_cost)

# Sample a subset (to keep Shapley computation practical)
X_sample_cost <- X_cost[1:100, ]
shapley_list_cost <- lapply(1:nrow(X_sample_cost), function(i) {
  Shapley$new(predictor_cost, x.interest = X_sample_cost[i, ])$results %>%
    mutate(instance = i)
})
shap_long_cost <- do.call(rbind, shapley_list_cost)

# Plot: violin distribution (same style as death model)
ggplot(shap_long_cost, aes(x = phi, y = feature, fill = feature)) +
  geom_violin(trim = FALSE, scale = "width", color = NA, alpha = 0.8) +
  geom_vline(xintercept = 0, linetype = "dotted", color = "gray50") +
  labs(
    title = "SHAP Value Distribution per Feature (Cost Model)",
    x = "SHAP Value (Impact on Prediction)",
    y = NULL
  ) +
  theme_minimal(base_family = "Times") +
  theme(
    legend.position = "none",
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.text.y = element_text(size = 9)
  ) +
  scale_fill_manual(
    values = rep(c("#f6765b", "#c7dcff", "#FABD05", "#ffe1fc"), length.out = length(unique(shap_long_cost$feature)))
  )



# Permutation Test for Cost Model

set.seed(42)
n_permutations <- 30
r2_null_cost <- numeric(n_permutations)

for (i in 1:n_permutations) {
  # Permute target
  train_cost_permuted <- train_all
  train_cost_permuted$log_total_damage <- sample(train_all$log_total_damage)
  
  # Train RF on permuted target
  rf_null_cost <- caret::train(
    form,
    data = train_cost_permuted,
    method = "rf",
    trControl = ctrl,
    tuneGrid = rf_grid,
    importance = TRUE
  )
  
  # Predict on the *real* test set
  preds_null_cost <- predict(rf_null_cost, newdata = test_all)
  
  # Evaluate R²
  r2_null_cost[i] <- cor(preds_null_cost, test_all$log_total_damage, use = "complete.obs")^2
}

# Real model R²
real_r2_cost <- eval_all["R2"]
cat("Real model R²:", real_r2_cost, "\n")
cat("Null mean R²:", mean(r2_null_cost), "\n")


df_r2_cost <- data.frame(R2 = r2_null_cost)
dens_cost <- density(r2_null_cost)
df_dens_cost <- data.frame(x = dens_cost$x, y = dens_cost$y)

ggplot(df_dens_cost, aes(x = x, y = y)) +
  geom_area(fill = "#ffe1fc", alpha = 0.8, color = "gray40", linewidth = 0.5) +
  geom_vline(xintercept = real_r2_cost, color = "#EA4336", linetype = "dashed", linewidth = 1.2) +
  geom_text(
    aes(x = real_r2_cost, y = max(y) * 0.9),
    label = paste("Real R² =", round(real_r2_cost, 3)),
    hjust = -0.1,
    color = "#EA4336",
    fontface = "bold",
    size = 4
  ) +
  labs(
    title = "Permutation Test: R² of Null Models vs Real Model (Cost Model)",
    subtitle = paste0("Real R² = ", round(real_r2_cost, 3),
                      " | Null mean R² = ", round(mean(r2_null_cost), 3)),
    x = "R² from Permuted Targets",
    y = "Density"
  ) +
  theme_minimal(base_family = "Times") +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 10, hjust = 0.5),
    axis.title = element_text(face = "bold")
  )





